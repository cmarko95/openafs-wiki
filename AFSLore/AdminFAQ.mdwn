## <a name="3  AFS administration"></a> 3 AFS administration

The Administration Section of the [[AFSFrequentlyAskedQuestions]].

- [[PreambleFAQ]]
- [[GeneralFAQ]]
- [[UsageFAQ]]

<div>
  <ul>
    <li><a href="#3  AFS administration"> 3 AFS administration</a><ul>
        <li><a href="#3.01  Is there a version of xdm"> 3.01 Is there a version of xdm available with AFS authentication?</a></li>
        <li><a href="#3.02  Is there a version of xloc"> 3.02 Is there a version of xlock available with AFS authentication?</a></li>
        <li><a href="#3.03  What is /afs/@cell?"> 3.03 What is /afs/@cell?</a></li>
        <li><a href="#3.04  Given that AFS data is loc"> 3.04 Given that AFS data is location independent, how does an AFS client determine which server houses the data its user is attempting to access?</a></li>
        <li><a href="#3.05  How does AFS maintain cons"> 3.05 How does AFS maintain consistency on read-write files?</a></li>
        <li><a href="#3.06  Which protocols does AFS u"> 3.06 Which protocols does AFS use?</a></li>
        <li><a href="#3.07  Which TCP/IP ports and pro"> 3.07 Which TCP/IP ports and protocols do I need to enable in order to operate AFS through my Internet firewall?</a></li>
        <li><a href="#3.08  Are setuid programs execut"> 3.08 Are setuid programs executable across AFS cell boundaries?</a></li>
        <li><a href="#3.09  How can I run daemons with"> 3.09 How can I run daemons with tokens that do not expire?</a></li>
        <li><a href="#3.10  Can I check my user's pass"> 3.10 Can I check my user's passwords for security purposes?</a></li>
        <li><a href="#3.11  Is there a way to automati"> 3.11 Is there a way to automatically balance disk usage across fileservers?</a></li>
        <li><a href="#3.12  Can I shutdown an AFS file"> 3.12 Can I shutdown an AFS fileserver without affecting users?</a></li>
        <li><a href="#3.13  How can I set up mail deli"> 3.13 How can I set up mail delivery to users with $HOMEs in AFS?</a></li>
        <li><a href="#3.14  Should I replicate a _Read"> 3.14 Should I replicate a ReadOnly volume on the same partition and server as the ReadWrite volume?</a></li>
        <li><a href="#3.15  Should I start AFS before"> 3.15 Should I start AFS before NFS in /etc/inittab?</a></li>
        <li><a href="#3.16  Will AFS run on a multi-ho"> 3.16 Will AFS run on a multi-homed fileserver?</a></li>
        <li><a href="#3.17  Can I replicate my user's"> 3.17 Can I replicate my user's home directory AFS volumes?</a></li>
        <li><a href="#3.18  What is the Andrew Benchma"> 3.18 What is the Andrew Benchmark?</a></li>
        <li><a href="#3.19  Where can I find the Andre"> 3.19 Where can I find the Andrew Benchmark?</a></li>
        <li><a href="#3.20  Is there a version of HP V"> 3.20 Is there a version of HP VUE login with AFS authentication?</a></li>
        <li><a href="#3.21  How can I list which clien"> 3.21 How can I list which clients have cached files from a server?</a></li>
        <li><a href="#3.22  Do Backup volumes require"> 3.22 Do Backup volumes require as much space as ReadWrite volumes?</a></li>
        <li><a href="#3.23  Should I run timed on my A"> 3.23 Should I run timed on my AFS client?</a></li>
        <li><a href="#3.24  Why should I keep /usr/vic"> 3.24 Why should I keep /usr/vice/etc/CellServDB current?</a></li>
        <li><a href="#3.25  How can I keep /usr/vice/e"> 3.25 How can I keep /usr/vice/etc/CellServDB current?</a></li>
        <li><a href="#3.26  How can I compute a list o"> 3.26 How can I compute a list of AFS fileservers?</a></li>
        <li><a href="#3.27  How can I set up anonymous"> 3.27 How can I set up anonymous FTP login to access /afs?</a></li>
        <li><a href="#3.28 Is the data sent over the n"> 3.28 Is the data sent over the network encrypted in AFS ?</a></li>
        <li><a href="#3.29 What underlying filesystems"> 3.29 What underlying filesystems can I use for AFS ?</a></li>
        <li><a href="#3.30 Compiling _OpenAFS"> 3.30 Compiling OpenAFS</a></li>
        <li><a href="#3.31 Upgrading _OpenAFS"> 3.31 Upgrading OpenAFS</a></li>
        <li><a href="#3.32 Debugging _OpenAFS"> 3.32 Debugging OpenAFS</a></li>
        <li><a href="#3.33 Tuning client cache for hug"> 3.33 Tuning client cache for huge data</a></li>
        <li><a href="#3.34 Settting up PAM with AFS"> 3.34 Settting up PAM with AFS</a></li>
        <li><a href="#3.35 Setting up AFS key in KDC a"> 3.35 Setting up AFS key in KDC and KeyFile</a></li>
        <li><a href="#3.36 Obtaining and getting asetk"> 3.36 Obtaining and getting asetkey compiled</a></li>
        <li><a href="#3.37 afs_krb_get_lrealm() using"> 3.37 afs_krb_get_lrealm() using /usr/afs/etc/krb.conf</a></li>
        <li><a href="#3.38 Moving from kaserver to Hei"> 3.38 Moving from kaserver to Heimdal KDC</a></li>
        <li><a href="#3.39 Moving from KTH-KRB4 to Hei"> 3.39 Moving from KTH-KRB4 to Heimdal KDC</a></li>
        <li><a href="#3.40 What are those bos(1) -type"> 3.40 What are those bos(1) -type values simple and cron?</a></li>
        <li><a href="#3.41 KDC listens on port 88 inst"> 3.41 KDC listens on port 88 instead of 750</a></li>
        <li><a href="#3.42 afsd gives me "Error -1 in"> 3.42 afsd gives me "Error -1 in basic initialization." on startup</a></li>
        <li><a href="#3.43 Although I get krb tickets,"> 3.43 Although I get krb tickets, afslog doesn't give me tokens, I see UDP packets to port 4444</a></li>
        <li><a href="#3.44 I get error message trhough"> 3.44 I get error message trhough syslogd: "afs: Tokens for user of AFS id 0 for cell foo.bar.baz are discarded (rxkad error=19270407)"</a></li>
        <li><a href="#3.45 I get tickets and tokens, b"> 3.45 I get tickets and tokens, but still get Permission denied.</a></li>
      </ul>
    </li>
  </ul>
</div>

- [[ResourcesFAQ]]
- [[AboutTheFAQ]]
- [[FurtherReading]]

### <a name="3.01  Is there a version of xdm"></a><a name="3.01  Is there a version of xdm "></a> 3.01 Is there a version of xdm available with AFS authentication?

Yes, xdm can be found in:

<file:///afs/transarc.com/public/afs-contrib/tools/xdm> <ftp://ftp.transarc.com/pub/afs-contrib/tools/xdm/MANIFEST>

### <a name="3.02  Is there a version of xloc"></a> 3.02 Is there a version of xlock available with AFS authentication?

Yes, xlock can be found in:

<file:///afs/transarc.com/public/afs-contrib/tools/xlock> <ftp://ftp.transarc.com/pub/afs-contrib/tools/xlock/MANIFEST>

### <a name="3.03  What is /afs/@cell?"></a> 3.03 What is /afs/@cell?

It is a symbolic link pointing at /afs/$your\_cell\_name.

NB, @cell is not something that is provided by AFS. You may decide it is useful in your cell and wish to create it yourself.

/afs/@cell is useful because:

- If you look after more than one AFS cell, you could create the link in each cell then set your PATH as:
  - PATH=$PATH:/afs/@cell/@sys/local/bin

- For most cells, it shortens the path names to be typed in thus reducing typos and saving time.

A disadvantage of using this convention is that when you cd into /afs/@cell then type "pwd" you see "/afs/@cell" instead of the full name of your cell. This may appear confusing if a user wants to tell a user in another cell the pathname to a file.

You could create your own /afs/@cell with the following:

    #/bin/ksh -
    # author: mpb
    [ -L /afs/@cell ] && echo We already have @cell! && exit
    cell=$(cat /usr/vice/etc/ThisCell)
    cd /afs/.${cell} && fs mkm temp root.afs
    cd temp
    ln -s /afs/${cell} @cell
    ln -s /afs/.${cell} .@cell            # .@cell for RW path
    cd /afs/.${cell} && fs rmm temp
    vos release root.afs; fs checkv

<http://www-archive.stanford.edu/lists/info-afs/hyper95/0298.html>

### <a name="3.04  Given that AFS data is loc"></a> 3.04 Given that AFS data is location independent, how does an AFS client determine which server houses the data its user is attempting to access?

The Volume Location Database (VLDB) is stored on AFS Database Servers and is ideally replicated across 3 or more Database Server machines. Replication of the Database ensures high availability and load balances the requests for the data. The VLDB maintains information regarding the current physical location of all volume data (files and directories) in the cell, including the IP address of the [[FileServer]], and the name of the disk partition the data is stored on.

A list of a cell's Database Servers is stored on the local disk of each AFS Client machine as: /usr/vice/etc/CellServDB

The Database Servers also house the Kerberos Authentication Database (encrypted user and server passwords), the Protection Database (user UID and protection group information) and the Backup Database (used by System Administrators to backup AFS file data to tape).

### <a name="3.05  How does AFS maintain cons"></a> 3.05 How does AFS maintain consistency on read-write files?

AFS uses a mechanism called "callback".

Callback is a promise from the fileserver that the cache version of a file/directory is up-to-date. It is established by the fileserver with the caching of a file.

When a file is modified the fileserver breaks the callback. When the user accesses the file again the Cache Manager fetches a new copy if the callback has been broken.

The following paragraphs describe AFS callback mechanism in more detail:

If I open() fileA and start reading, and you then open() fileA, write() a change **\*\*and close() or fsync()\*\*** the file to get your changes back to the server - at the time the server accepts and writes your changes to the appropriate location on the server disk, the server also breaks callbacks to all clients to which it issued a copy of fileA.

So my client receives a message to break the callback on fileA, which it dutifully does. But my application (editor, spreadsheet, whatever I'm using to read fileA) is still running, and doesn't really care that the callback has been broken.

When something causes the application to read() more of the file the read() system call executes AFS cache manager code via the VFS switch, which does check the callback and therefore gets new copies of the data.

Of course, the application may not re-read data that it has already read, but that would also be the case if you were both using the same host. So, for both AFS and local files, I may not see your changes.

Now if I exit the application and start it again, or if the application does another open() on the file, then I will see the changes you've made.

This information tends to cause tremendous heartache and discontent - but unnecessarily so. People imagine rampant synchronization problems. In practice this rarely happens and in those rare instances, the data in question is typically not critical enough to cause real problems or crashing and burning of applications. Since 1985, we've found that the synchronization algorithm has been more than adequate in practice - but people still like to worry!

The source of worry is that, if I make changes to a file from my workstation, your workstation is not guaranteed to be notified until I close or fsync the file, at which point AFS guarantees that your workstation will be notified. This is a significant departure from NFS, in which no guarantees are provided.

Partially because of the worry factor and largely because of Posix, this will change in DFS. DFS synchronization semantics are identical to local file system synchronization.

[ DFS is the Distributed File System which is part of the Distributed ] [ Computing Environment (DCE). ]

### <a name="3.06  Which protocols does AFS u"></a> 3.06 Which protocols does AFS use?

AFS may be thought of as a collection of protocols and software processes, nested one on top of the other. The constant interaction between and within these levels makes AFS a very sophisticated software system.

At the lowest level is the UDP protocol, which is part of TCP/IP. UDP is the connection to the actual network wire. The next protocol level is the remote procedure call (RPC). In general, RPCs allow the developer to build applications using the client/server model, hiding the underlying networking mechanisms. AFS uses Rx, an RPC protocol developed specifically for AFS during its development phase at Carnegie Mellon University.

Above the RPC is a series of server processes and interfaces that all use Rx for communication between machines. Fileserver, volserver, upserver, upclient, and bosserver are server processes that export RPC interfaces to allow their user interface commands to request actions and get information. For example, a bos status  command will examine the bos server process on the indicated file server machine.

Database servers use ubik, a replicated database mechanism which is implemented using RPC. Ubik guarantees that the copies of AFS databases of multiple server machines remain consistent. It provides an application programming interface (API) for database reads and writes, and uses RPCs to keep the database synchronized. The database server processes, vlserver, kaserver, and ptserver, reside above ubik. These processes export an RPC interface which allows user commands to control their operation. For instance, the pts command is used to communicate with the ptserver, while the command klog uses the kaserver's RPC interface.

Some application programs are quite complex, and draw on RPC interfaces for communication with an assortment of processes. Scout utilizes the RPC interface to file server processes to display and monitor the status of file servers. The uss command interfaces with kaserver, ptserver, volserver and vlserver to create new user accounts.

The Cache Manager also exports an RPC interface. This interface is used principally by file server machines to break callbacks. It can also be used to obtain Cache Manager status information. The program cmdebug shows the status of a Cache Manager using this interface.

For additional information, Section 1.5 of the AFS System Administrator's Guide and the April 1990 Cache Update contain more information on ubik. Udebug information and short descriptions of all debugging tools were included in the January 1991 Cache Update. Future issues will discuss other debugging tools in more detail.

[ source: <ftp://ftp.transarc.com/pub/afsug/newsletter/apr91> ] [ Copyright 1991 Transarc Corporation ]

### <a name="3.07  Which TCP/IP ports and pro"></a> 3.07 Which TCP/IP ports and protocols do I need to enable in order to operate AFS through my Internet firewall?

Assuming you have already taken care of nameserving, you may wish to use an Internet timeserver for Network Time Protocol [[[NTP|Main/FurtherReading#NTP]]] and the question about [[timed|Main/WebHome#NTP]]:

ntp 123/udp

A list of NTP servers is available via anonymous FTP from:

- <http://www.eecis.udel.edu/~mills/ntp/servers.html>

For further details on NTP see: <http://www.eecis.udel.edu/~ntp/>

For a "minimal" AFS service which does not allow inbound or outbound klog:

       cachemanager    4711/udp (only if you use the arla-client instead of OpenAFS)
       fileserver      7000/udp
       cachemanager    7001/udp
       ptserver        7002/udp
       vlserver        7003/udp
       kaserver        7004/udp
       volserver       7005/udp
       reserved        7006/udp
       bosserver       7007/udp

(Ports in the 7020-7029 range are used by the AFS backup system, and won't be needed by external clients performing simple file accesses.)

Additionally, for "klog" to work through the firewall you need to allow inbound and outbound UDP on ports &gt;1024 (probably 1024&lt;port&lt;2048 would suffice depending on the number of simultaneous klogs).

See also: <http://www-archive.stanford.edu/lists/info-afs/hyper95/0874.html>

### <a name="3.08  Are setuid programs execut"></a> 3.08 Are setuid programs executable across AFS cell boundaries?

By default, the setuid bit is ignored but the program may be run (without setuid privilege).

It is possible to configure an AFS client to honour the setuid bit. This is achieved by root running:

       root@toontown # fs setcell -cell $cellname -suid

(where $cellname is the name of the foreign cell. Use with care!).

NB: making a program setuid (or setgid) in AFS does **not** mean that the program will get AFS permissions of a user or group. To become AFS authenticated, you have to klog. If you are not authenticated, AFS treats you as "system:anyuser".

### <a name="3.09  How can I run daemons with"></a> 3.09 How can I run daemons with tokens that do not expire?

It is not a good idea to run with tokens that do not expire because this would weaken one of the security features of Kerberos.

A better approach is to re-authenticate just before the token expires.

There are two examples of this that have been contributed to afs-contrib. The first is "reauth":

- <file:///afs/transarc.com/public/afs-contrib/tools/reauth/>
- <ftp://ftp.transarc.com/pub/afs-contrib/tools/reauth/MANIFEST>
- <ftp://ftp.andrew.cmu.edu/pub/AFS-Tools/reauth-0.0.5.tar.gz>

The second is "lat":

<file:///afs/transarc.com/public/afs-contrib/pointers/UMich-lat-authenticated-batch-jobs> <ftp://ftp.transarc.com/pub/afs-contrib/pointers/UMich-lat-authenticated-batch-jobs>

Another collection of tools was [mentioned](https://lists.openafs.org/pipermail/openafs-info/2002-October/006353.html) by [[DanielClark]]:

Another option is [OpenPBS](http://www.openpbs.org/) and [Password Storage and Retrieval](http://www.lam-mpi.org/software/psr/) (PSR), where you encrypt your AFS password with a public key and put it in your home directory, and trusted machine(s) which have the private key on local disk then decrypt your password and run your job. MIT uses a variant of this (e.g. [a](http://web.mit.edu/longjobs/www/) &amp; [b](http://mit.edu/longjobs-dev/notebook/)) that uses their own code (see [longjobs documentation](http://web.mit.edu/longjobs-dev/doc/netsec.txt) sections III and IV) instead of PSR.

### <a name="3.10  Can I check my user&#39;s pass"></a> 3.10 Can I check my user's passwords for security purposes?

Yes. Alec Muffett's Crack tool (at version 4.1f) has been converted to work on the Transarc kaserver database. This modified Crack (AFS Crack) is available via anonymous ftp from:

- <ftp://export.acs.cmu.edu/pub/crack.tar.Z>
- <ftp://ftp.andrew.cmu.edu/pub/AFS-Tools/crack.tar.Z>

and is known to work on: pmax\_\* sun4\*\_\* hp700\_\* rs\_aix\* next\_\*

It uses the file /usr/afs/db/kaserver.DB0, which is the database on the kaserver machine that contains the encrypted passwords. As a bonus, AFS Crack is usually two to three orders of magnitude faster than the standard Crack since there is no concept of salting in a Kerberos database.

On a normal UNIX /etc/passwd file, each password can have been encrypted around 4096 (2^12) different saltings of the crypt(3) algorithm, so for a large number of users it is easy to see that a potentially large (up to 4095) number of seperate encryptions of each word checked has been avoided.

Author: Dan Lovinger Contact: Derrick J. Brashear &lt;shadow+@andrew.cmu.edu&gt;

<table border="1" cellpadding="0" cellspacing="0">
  <tr>
    <td> Note: </td>
    <td> AFS Crack does not work for MIT Kerberos Databases. The author is willing to give general guidance to someone interested in doing the (probably minimal) amount of work to port it to do MIT Kerberos. The author does not have access to a MIT Kerberos server to do this. </td>
  </tr>
</table>

### <a name="3.11  Is there a way to automati"></a> 3.11 Is there a way to automatically balance disk usage across fileservers?

Yes. There is a tool, balance, which does exactly this. It can be retrieved via anonymous ftp from:

- <ftp://ftp.andrew.cmu.edu/pub/AFS-Tools/balance-1.1b.tar.gz>

Actually, it is possible to write arbitrary balancing algorithms for this tool. The default set of "agents" provided for the current version of balance balance by usage, # of volumes, and activity per week, the latter currently requiring a source patch to the AFS volserver. Balance is highly configurable.

Author: Dan Lovinger Contact: Derrick Brashear &lt;shadow+@andrew.cmu.edu&gt;

### <a name="3.12  Can I shutdown an AFS file"></a> 3.12 Can I shutdown an AFS fileserver without affecting users?

Yes, this is an example of the flexibility you have in managing AFS.

Before attempting to shutdown an AFS fileserver you have to make some arrangements that any services that were being provided are moved to another AFS fileserver:

1. Move all AFS volumes to another fileserver. (Check you have the space!) This can be done "live" while users are actively using files in those volumes with no detrimental effects.

1. Make sure that critical services have been replicated on one (or more) other fileserver(s). Such services include:
  - kaserver - Kerberos Authentication server
  - vlserver - Volume Location server
  - ptserver - Protection server
  - buserver - Backup server

It is simple to test this before the real shutdown by issuing:

       bos shutdown $server $service

       where: $server is the name of the server to be shutdown
         and  $service is one (or all) of: kaserver vlserver ptserver buserver

Other points to bear in mind:

- "vos remove" any RO volumes on the server to be shutdown. Create corresponding RO volumes on the 2nd fileserver after moving the RW. There are two reasons for this:
  1. An RO on the same partition ("cheap replica") requires less space than a full-copy RO.
  2. Because AFS always accesses RO volumes in preference to RW, traffic will be directed to the RO and therefore quiesce the load on the fileserver to be shutdown.

- If the system to be shutdown has the lowest IP address there may be a brief delay in authenticating because of timeout experienced before contacting a second kaserver.

### <a name="3.13  How can I set up mail deli"></a> 3.13 How can I set up mail delivery to users with $HOMEs in AFS?

There are many ways to do this. Here, only two methods are considered:

Method 1: deliver into local filestore

This is the simplest to implement. Set up your mail delivery to append mail to /var/spool/mail/$USER on one mailserver host. The mailserver is an AFS client so users draw their mail out of local filestore into their AFS $HOME (eg: inc).

Note that if you expect your (AFS unauthenticated) mail delivery program to be able to process .forward files in AFS $HOMEs then you need to add "system:anyuser rl" to each $HOMEs ACL.

The advantages are:

- Simple to implement and maintain.
- No need to authenticate into AFS.

The drawbacks are:

- It doesn't scale very well.
- Users have to login to the mailserver to access their new mail.
- Probably less secure than having your mailbox in AFS.
- System administrator has to manage space in /var/spool/mail.

Method 2: deliver into AFS

This takes a little more setting up than the first method.

First, you must have your mail delivery daemon AFS authenticated (probably as "postman"). The reauth example in afs-contrib shows how a daemon can renew its token. You will also need to setup the daemon startup soon after boot time to klog (see the -pipe option).

Second, you need to set up the ACLs so that "postman" has lookup rights down to the user's $HOME and "lik" on $HOME/Mail.

Advantages:

- Scales better than first method.
- Delivers to user's $HOME in AFS giving location independence.
- Probably more secure than first method.
- User responsible for space used by mail.

Disadvantages:

- More complicated to set up.
- Need to correctly set ACLs down to $HOME/Mail for every user.
- Probably need to store postman's password in a file so that the mail delivery daemon can klog after boot time. This may be OK if the daemon runs on a relatively secure host.

An example of how to do this for IBM RISC System/6000 is auth-sendmail. A beta test version of auth-sendmail can be found in:

<file:///afs/transarc.com/public/afs-contrib/doc/faq/auth-sendmail.tar.Z> <ftp://ftp.transarc.com/pub/afs-contrib/doc/faq/auth-sendmail.tar.Z>

### <a name="3.14  Should I replicate a _Read"></a> 3.14 Should I replicate a [[ReadOnly]] volume on the same partition and server as the [[ReadWrite]] volume?

Yes, Absolutely! It improves the robustness of your served volumes.

If [[ReadOnly]] volumes exist (note use of term **exist** rather than **are available**), Cache Managers will **never** utilize the [[ReadWrite]] version of the volume. The only way to access the RW volume is via the "dot" path (or by special mounting).

This means if **all** RO copies are on dead servers, are offline, are behind a network partition, etc, then clients will not be able to get the data, even if the RW version of the volume is healthy, on a healthy server and in a healthy network.

However, you are **very** strongly encouraged to keep one RO copy of a volume on the **same server and partition** as the RW. There are two reasons for this:

1. The RO that is on the same server and partition as the RW is a clone (just a copy of the header - not a full copy of each file). It therefore is very small, but provides access to the same set of files that all other (full copy) [[ReadOnly]] volume do. Transarc trainers refer to this as the "cheap replica".
2. To prevent the frustration that occurs when all your ROs are unavailable but a perfectly healthy RW was accessible but not used.

If you keep a "cheap replica", then by definition, if the RW is available, one of the RO's is also available, and clients will utilize that site.

### <a name="3.15  Should I start AFS before"></a><a name="3.15  Should I start AFS before "></a> 3.15 Should I start AFS before NFS in /etc/inittab?

Yes, it is possible to run both AFS and NFS on the same system but you should start AFS first.

In IBM's AIX 3.2, your /etc/inittab would contain:

    rcafs:2:wait:/etc/rc.afs > /dev/console 2>&1 # Start AFS daemons
    rcnfs:2:wait:/etc/rc.nfs > /dev/console 2>&1 # Start NFS daemons

With AIX, you need to load NFS kernel extensions before the AFS KEs in /etc/rc.afs like this:

    #!/bin/sh -
    # example /etc/rc.afs for an AFS fileserver running AIX 3.2
    #
    echo "Installing NFS kernel extensions (for AFS+NFS)"
    /etc/gfsinstall -a /usr/lib/drivers/nfs.ext
    echo "Installing AFS kernel extensions..."
    D=/usr/afs/bin/dkload
    ${D}/cfgexport -a ${D}/export.ext
    ${D}/cfgafs    -a ${D}/afs.ext
    /usr/afs/bin/bosserver &

### <a name="3.16  Will AFS run on a multi-ho"></a> 3.16 Will AFS run on a multi-homed fileserver?

(multi-homed = host has more than one network interface.)

Yes, it will. However, AFS was designed for hosts with a single IP address. There can be problems if you have one host name being resolved to several IP addresses.

Transarc suggest designating unique hostnames for each network interface. For example, a host called "spot" has two tokenring and one ethernet interfaces: spot-tr0, spot-tr1, spot-en0. Then, select which interface will be used for AFS and use that hostname in the [[CellServDB]] file (eg: spot-tr0).

You also have to remember to use the AFS interface name with any AFS commands that require a server name (eg: vos listvol spot-tr0).

There is a more detailed discussion of this in the August 1993 issue of "Cache Update" (see: <ftp://ftp.transarc.com/pub/afsug/newsletter/aug93>).

The simplest way of dealing with this is to make your AFS fileservers single-homed (eg only use one network interface).

At release 3.4 of AFS, it is possible to have multi-homed fileservers (but _not_ multi-homed database servers).

### <a name="3.17  Can I replicate my user&#39;s"></a><a name="3.17  Can I replicate my user&#39;s "></a> 3.17 Can I replicate my user's home directory AFS volumes?

No.

Users with $HOMEs in /afs normally have an AFS [[ReadWrite]] volume mounted in their home directory.

You can replicate a RW volume but only as a [[ReadOnly]] volume and there can only be one instance of a [[ReadWrite]] volume.

In theory, you could have RO copies of a user's RW volume on a second server but in practice this won't work for the following reasons:

    a) AFS has built-in bias to always access the RO copy of a RW volume.
       So the user would have a ReadOnly $HOME which is not too useful!

    b) Even if a) was not true you would have to arrange frequent
       synchronisation of the RO copy with the RW volume (for example:
       "vos release user.fred; fs checkv") and this would have to be
       done for all such user volumes.

    c) Presumably, the idea of replicating is to recover the $HOME
       in the event of a server crash. Even if a) and b) were not
       problems consider what you might have to do to recover a $HOME:

       1) Create a new RW volume for the user on the second server
          (perhaps named "user.fred.2").

       2) Now, where do you mount it?

          The existing mountpoint cannot be used because it already has
          the ReadOnly copy of the original volume mounted there.

          Let's choose: /afs/MyCell/user/fred.2

       3) Copy data from the RO of the original into the new RW volume
          user.fred.2

       4) Change the user's entry in the password file for the new $HOME:
          /afs/MyCell/user/fred.2

       You would have to attempt steps 1 to 4 for every user who had
       their RW volume on the crashed server. By the time you had done
       all of this, the crashed server would probably have rebooted.

       The bottom line is: you cannot replicate $HOMEs across servers.

### <a name="3.18  What is the Andrew Benchma"></a> 3.18 What is the Andrew Benchmark?

"It is a script that operates on a collection of files constituting an application program. The operations are intended to represent typical actions of an average user. The input to the benchmark is a source tree of about 70 files. The files total about 200 KB in size. The benchmark consists of five distinct phases:

      I MakeDir - Construct a target subtree that is identical to the
                  source subtree.
     II Copy    - Copy every file from the source subtree to the target subtree.
    III ScanDir - Traverse the target subtree and examine the status
                  of every file in it.
     IV ReadAll - Scan every byte of every file in the target subtree.
      V Make    - Complete and link all files in the target subtree."

Source: <file:///afs/transarc.com/public/afs-contrib/doc/benchmark/Andrew.Benchmark.ps> <ftp://ftp.transarc.com/pub/afs-contrib/doc/benchmark/Andrew.Benchmark.ps>

### <a name="3.19  Where can I find the Andre"></a> 3.19 Where can I find the Andrew Benchmark?

From Daniel Joseph Barnhart Clark's [message](http://lists.openafs.org/pipermail/openafs-info/2004-April/012942.html):

It's linked to from <http://www.citi.umich.edu/projects/linux-scalability/tools.html>; CITI also has some useful mods at <http://www.citi.umich.edu/projects/nfs-perf/results/cmarion/>.

### <a name="3.20  Is there a version of HP V"></a> 3.20 Is there a version of HP VUE login with AFS authentication?

Yes, the availability of this is described in: <file:///afs/transarc.com/public/afs-contrib/pointers/HP-VUElogin.txt> <ftp://ftp.transarc.com/pub/afs-contrib/pointers/HP-VUElogin.txt>

If you don't have access to the above, please contact Rajeev Pandey of Hewlett Packard whose email address is &lt;rpandey@cv.hp.com&gt;.

### <a name="3.21  How can I list which clien"></a> 3.21 How can I list which clients have cached files from a server?

By using the following script:

    #!/bin/ksh -
    #
    # NAME          afsclients
    # AUTHOR        Rainer Toebbicke  <rtb@dxcern.cern.ch>
    # DATE          June 1994
    # PURPOSE       Display AFS clients which have grabbed files from a server

    if [ $# = 0 ]; then
            echo "Usage: $0 <afs_server 1> ... <afsserver n>"
            exit 1
    fi
    for n; do
            /usr/afsws/etc/rxdebug -servers $n -allconn
    done | grep '^Connection' | \
    while  read x y z ipaddr rest; do echo $ipaddr; done | sort -u |
    while read ipaddr; do
            ipaddr=${ipaddr%%,}
            n="`nslookup $ipaddr`"
            n="${n##*Name: }"
            n="${n%%Address:*}"
            n="${n##*([ ])}"
            n="${n%?}"
            echo "$n ($ipaddr)"
    done

### <a name="3.22  Do Backup volumes require"></a><a name="3.22  Do Backup volumes require "></a> 3.22 Do Backup volumes require as much space as [[ReadWrite]] volumes?

No.

The technique used is to create a new volume, where every file in the RW copy is pointed to by the new backup volume. The files don't exist in the BK, only in the RW volume. The backup volume therefore takes up very little space.

If the user now starts modifying data, the old copy must not be destroyed.

There is a Copy-On-Write bit in the vnode - if the fileserver writes to a vnode with the bit on it allocates a new vnode for the data and turns off the COW bit. The BK volume hangs onto the old data, and the RW volume slowly splits itself away over time.

The BK volume is re-synchronised with the RW next time a "vos backupsys" is run.

The space needed for the BK volume is directly related to the size of all files changed in the RW between runs of "vos backupsys".

### <a name="3.23  Should I run timed on my A"></a> 3.23 Should I run timed on my AFS client?

No.

<a name="NTP"></a> The AFS Servers make use of NTP [[[NTP|Main/FurtherReading#NTP]]] to synchronise time each other and typically with one or more external NTP servers. By default, clients synchronize their time with one of the servers in the local cell. Thus all the machines participating in the AFS cell have an accurate view of the time.

For further details on NTP see: <http://www.eecis.udel.edu/~ntp/>. The latest version is 4.1, dated August 2001, which is **much** more recent that the version packaged with Transarc AFS.

A list of NTP servers is available via anonymous FTP from:

- <http://www.eecis.udel.edu/~mills/ntp/servers.html>

The default time setting behavior of the AFS client can be disabled by specifying the `-nosettime` argument to [afsd](http://www.transarc.ibm.com/Library/documentation/afs/3.5/unix/cmd/cmd53.htm). This is recommended for AFS servers which are also configured as clients (because servers normally run NTP daemons) and for clients that run NTP.

### <a name="3.24  Why should I keep /usr/vic"></a> 3.24 Why should I keep /usr/vice/etc/CellServDB current?

On AFS clients, /usr/vice/etc/CellservDB, defines the cells and (their servers) that can be accessed via /afs.

Over time, site details change: servers are added/removed or moved onto new network addresses. New sites appear.

In order to keep up-to-date with such changes, the [[CellservDB]] file on each AFS client should be kept consistent with some master copy (at your site).

As well as updating [[CellservDB]], your AFS administrator should ensure that new cells are mounted in your cell's root.afs volume.

If a cell is added to [[CellServDB]] then the **client** must either be restared or you must the AFS command "fs newcell -cell .. -servers ... ...".

### <a name="3.25  How can I keep /usr/vice/e"></a> 3.25 How can I keep /usr/vice/etc/CellServDB current?

Do a daily copy from a master source and update the AFS kernel sitelist.

One good master source is the grand.central.org [[CellServDB]], available from <http://grand.central.org/dl/cellservdb/CellServDB> or <file:/afs/openafs.org/service/CellServDB> (N.B. update to /afs/grand.central.org path when available). You can send updates for this to <cellservdb@central.org>.

The client [[CellServDB]] file must not reside under /afs and is best located in local filespace.

Simply updating a client [[CellServDB]] file is not enough. You also need to update the AFS kernel sitelist by either: 1 rebooting the client or 1 running "fs newcell $cell\_name $server\_list" for each site in the [[CellServDB]] file.

A script to update the AFS kernel sitelist on a running system is newCellServDB.

<file:///afs/ece.cmu.edu/usr/awk/Public/newCellServDB> <ftp://ftp.ece.cmu.edu/pub/afs-tools/newCellServDB>

One way to distribute [[CellServDB]] is to have a root cron job on each AFS client copy the file then run newCellServDB.

Example:

    #!/bin/ksh -
    #
    # NAME       syncCellServDB
    # PURPOSE    Update local CellServDB file and update AFS kernel sitelist
    # USAGE      run by daily root cron job eg:
    #                    0 3 * * * /usr/local/sbin/syncCellServDB
    #
    # NOTE       "@cell" is a symbolic link to /afs/$this_cell_name

    src=/afs/@cell/service/etc/CellServDB
    dst=/usr/vice/etc/CellServDB
    xec=/usr/local/sbin/newCellServDB
    log=/var/log/syncCellServDB

    if [ -s ${src} ]; then
            if [ ${src} -nt ${dst} ]; then
                    cp $dst ${dst}- && cp $src $dst && $xec 2>&1 >$log
            else
                    echo "master copy no newer: no processing to be done" >$log
            fi
    else
            echo "zero length file: ${src}" >&2
    fi

### <a name="3.26  How can I compute a list o"></a> 3.26 How can I compute a list of AFS fileservers?

Here is a Bourne shell command to do it (it will work in GNU bash and the Korn shell, too):

       stimpy@nick $ vos listvldb -cell `cat /usr/vice/etc/ThisCell` \\
                     | awk '(/server/) {print $2}' | sort -u

### <a name="3.27  How can I set up anonymous"></a> 3.27 How can I set up anonymous FTP login to access /afs?

The easiest way on a primarily "normal" machine (where you don't want to have everything in AFS) is to actually mount root.cell under ~ftp, and then symlink /afs to ~ftp/afs or whatever. It's as simple as changing the mountpoint in /usr/vice/etc/cacheinfo and restarting afsd.

Note that when you do this, anon ftp users can go anywhere system:anyuser can (or worse, if you're using IP-based ACLs and the ftp host is PTS groups). The only "polite" solution I've arrived at is to have the ftp host machine run a minimal [[CellServDB]] and police my ACLs tightly.

Alternatively, you can make ~ftp an AFS volume and just mount whatever you need under that - this works well if you can keep everything in AFS, and you don't have the same problems with anonymous "escapes" into /afs.

Unless you need to do authenticating ftp, you are _strongly_ recommended using wu-ftpdv2.4 (or better).

### <a name="3.28 Is the data sent over the n"></a> 3.28 Is the data sent over the network encrypted in AFS ?

There is still no easy way to do this in Transarc AFS, but [[OpenAFS]] now has a "fs" subcommand to turn on encryption of regular file data sent and received by a client. This is a per client setting that persist until reboot. No server actions are needed to support this change. The syntax is:

- `fs setcrypt on`
- `fs setcrypt off`
- `fs getcrypt`

Note that this only encrypts network traffic between the client and server. The data on the server's disk is not encrypted nor is the data in the client's disk cache. The encryption algorithm used is [fcrypt](http://tedanderson.home.mindspring.com/fcrypt-paper.txt), which is a DES variant.

Getting encryption enabled by default:

- [[RedHat]] Linux: ([src](https://lists.openafs.org/pipermail/openafs-info/2002-July/005085.html)) change the last line of /etc/sysconfig/afs to `AFS_POST_INIT="/usr/bin/fs setcrypt on"`
- Windows ([src](https://lists.openafs.org/pipermail/openafs-info/2003-June/009416.html)) set the following registry value named `SecurityLevel` under `HKLM\SYSTEM\CurrentControlSet\Services\TransarcAFSDaemon\Parameters` to 2.

I have not tested either of these procedures. -- [[TedAnderson]] - 05 Jun 2003

### <a name="3.29 What underlying filesystems"></a> 3.29 What underlying filesystems can I use for AFS ?

See also [[SupportedConfigurations]].

You need to distinguish between the filesystem used by the file server to store the actual AFS data (by convention in /vicep?) and the filesystem used by the client cache manager to cache files.

Fileserver is started by bosserver. It depends, what ./configure switches were used during compilation from sources. To be always on the safe side, use --enable-namei-fileserver configure flag. That will give you fileserver binary which can act on any /vicep? partition regardless it's filesystem type. With the new namei file server you can basically use any filesystem you want. The namei file server does not do any fancy stuff behind the scenes but only accesses normal files (their names are a bit strange though).

The opposite to namei fileserver is inode based nameserver. According to openafs-devel email list, it gave on some Solaris box 10% speedup over the namei based server. The namei based fileserver cannot run on every filesystem, as it stores some internal afs-specific data into the filesystem. Typically, /sbin/fsck distributed withthe operating system zaps these data. Inode based fileserver directly operates on the inodes of the underlying file system which therefore has to fully support the inode abstraction scheme. The the Administrators Guide for more details (they differ from system to system).

On the client side where you run /usr/afs/etc/afsd as a kernel process, you always have to use a file system supporting the inode abstraction for the cache (usually /usr/vice/cache) since the cache manager references files by their inode. Fortunately, it does not do such tricky stuff as the inode based fileserver.

The following file systems have been reported _not_ to work for the AFS client cache:

- reiserfs
- vxfs (HP-UX)
- advfs (Tru64), it works but gives cachecurruption
- efs (SGI) - IBM AFS does support efs, but openafs doesn't have a license for that.

- Patch committed to cvs around 6/2003 will now enforce this in some cases and generate a warning to the user if the filesystem type is wrong.

The following file systems have been reported to work for the AFS client cache:

- ext2
- ext3
- hfs (HP-UX)
- xfs (at least on IRIX 6.5)
- ufs (Solaris, [[Tru64Unix]])

### <a name="3.30 Compiling _OpenAFS"></a> 3.30 Compiling [[OpenAFS]]

The kernel parts on Solaris have to be compiled with Sun cc, same for other platforms, i.e. you need same compiler used to compile kernel to compile your afs modules. [[Tru64Unix]] doesn't support modules, so you have to edit kernel config files and link statically into kernel. Kernel module insertion works fine on Linux, Solaris, Irix ...

    ./configure --enable-transarc-paths=/usr/etc --with-afs-sysname=i386_linux24
    make dest
    cd dest/i386_linux24

... and continue the install process described in IBM AFS documentation. If you do "make install", you will end up with some stuff installed into /usr/local but something not, regardless the --enable-transarc-paths option ... "make install" it's messy.

### <a name="3.31 Upgrading _OpenAFS"></a> 3.31 Upgrading [[OpenAFS]]

Upgrade of AFS on Linux

    /etc/rc.d/init.d/afs stop
    cd root.client/usr/vice/etc
    tar cvf - . | (cd /usr/vice/etc; tar xfp -)
    cp -p afs.rc /etc/rc.d/init.d/afs
    cp ../../../../lib/pam_afs.krb.so.1  /lib/security
    cd ../../../../root.server/usr/afs
    tar cvf - . | (cd /usr/afs; tar xfp -)
    # echo "auth sufficient /lib/security/pam_afs.so try_first_pass \
    ignore_root" >> /etc/pam.d/login
    cd /lib/security
    vim /etc/sysconfig/afs
    ln -s pam_afs.krb.so.1 pam_afs.so
    cd /etc/rc3.d
    ln -s ../init.d/afs S99afs
    cd ../rc0.d
    ln -s ../init.d/afs K01afs
    cp /usr/vice/etc/afs.conf /etc/sysconfig/afs
    /etc/rc.d/init.d/afs start

Upgrade of AFS on Solaris 2.6

    cd /etc/rc3.d/
    mv S20afs aS20afs
    init 6
    cd root.server/usr/afs
    tar cvf - ./bin | (cd /usr/afs; tar xfp -)
    cd ../../..
    cp root.client/usr/vice/etc/modload/libafs.nonfs.o /kernel/fs/afs
    cp root.server/etc/vfsck /usr/lib/fs/afs/fsck
    cd root.client/usr/vice
    tar cvf - ./etc | (cd /usr/vice; tar xf -)
    cd ../../..
    cp lib/pam_afs.krb.so.1 /usr/lib/security
    cp lib/pam_afs.so.1 /usr/lib/security
    cd /etc/rc3.d
    mv aS20afs S20afs
    init 6

Upgrade of AFS on Irix 6.5

    /etc/chkconfig -f afsserver off
    /etc/chkconfig -f afsclient off
    /etc/chkconfig -f afsml off
    /etc/chkconfig -f afsxnfs off
    /etc/reboot
    cd root.server/usr/afs
    tar cvf - ./bin | (cd /usr/afs; tar xfp -)
    cd ../../..
    cp root.client/usr/vice/etc/sgiload/libafs.IP22.nonfs.o /usr/vice/etc/sgiload
    echo "AFS will be compiled statically into kernel"
    echo "otherwise skip following lines and use chkconfig afsml on"
    cp root.client/bin/afs.sm  /var/sysgen/system
    cp root.client/bin/afs /var/sysgen/master.d
    echo "The next file comes from openafs-*/src/libafs/STATIC.*"
    cp root.client/bin/libafs.IP22.nonfs.a /var/sysgen/boot/afs.a
    cp /unix /unix_orig
    /etc/autoconfig
    echo "end of static kernel modifications"
    cd root.client/usr/vice/etc
    echo "Delete any of the modload/ files which don't fit your platform if you need space"
    echo "These files originate from openafs-*/src/libafs/MODLOAD.*"
    tar cvf - . | (cd /usr/vice/etc; tar xf -)
    /etc/chkconfig -f afsserver on
    /etc/chkconfig -f afsclient on
    # /etc/chkconfig -f afsml on - afs is compiled statically into kernel, so leave afsml off
    /etc/chkconfig -f afsml off
    /etc/chkconfig -f afsxnfs off
    /etc/reboot

Upgrade of AFS on [[Tru64Unix]]

    cd root.server/usr/afs/
    tar cvf - ./bin | (cd /usr/afs; tar xfp -)
    cd ../../../root.client/bin
    cp ./libafs.nonfs.o /usr/sys/BINARY/afs.mod
    ls -la /usr/sys/BINARY/afs.mod
    doconfig -c FOO
    cd ../../root.client/usr/vice
    cp etc/afsd /usr/vice/etc/afsd
    cp etc/C/afszcm.cat /usr/vice/etc/C/afszcm.cat

### <a name="3.32 Debugging _OpenAFS"></a> 3.32 Debugging [[OpenAFS]]

In case of troubles when you need only fileserver process to run (to be able to debug), run the lwp fileserver instead of the pthreads fileserver (src/viced/fileserver instead of src/tviced/fileserver if you have a buildtree handy):

cp src/viced/fileserver /usr/afs/bin (or wherever)

    bos restart calomys fs -local

... then attach with gdb

To debug if client running afsd kernel process talks to the servers from [[CellServDB]], do:

    tcpdump -vv -s 1500 port 7001

Other ports are:

    fileserver      7000/udp                        # fileport (FILESERVICE)
    afscb           7001/udp                        # kernel extensions
    afsprot         7002/udp                        # ptserver (PROTSERVICE)
    afsvldb         7003/udp                        # vlserver (VLDBSERVICE)
    afskauth        7004/udp                        # kaserver (KAUTHSERVICE)
    afsvol          7005/udp                        # volserver (VOLUMESERVICE)
    afserror        7006/udp                        # ERRORSERVICE
    afsnanny        7007/udp                        # bosserver (NANNYSERVICE)
    afsupdate       7008/udp                        # upserver (UPDATESERVICE)
    afsrmtsys       7009/udp                        # RMTSYSSERVICE

### <a name="3.33 Tuning client cache for hug"></a> 3.33 Tuning client cache for huge data

Use on afsd command line -chunk 17 or greater. Be carefull, with certain cache sizes afsd crashes on startup (Linux, [[Tru64Unix]] at least). It is possibly when dcache is too small. Go for:

    /usr/vice/etc/afsd -nosettime -stat 12384 -chunk 19

    > So I ran the full suite of iozone tests (13), but at a single file
    > size (128M) and one record size (64K).  I set the AFS cache size to > 80000K for both memcache and diskcache.

    Note that memcache size and diskcache size are different things.
    In the case of memcache, a fixed number of chunks are allocated
    in memory, such that numChunks * chunkSize = memCacheSize.  In
    the case of disk cache, there are a lot more chunks, because the
    disk cache assumes not every chunk will be filled (the underlying
    filesystem handles disk block allocation for us).  Thus, when you
    have small file segments, they use up an entire chunk worth of
    cache in the memcache case, but only their size worth of cache
    in the diskcache cache.

    -- kolya

### <a name="3.34 Settting up PAM with AFS"></a> 3.34 Settting up PAM with AFS

Solaris

    auth        sufficient    /lib/security/pam_afs.so debug try_first_pass ignore_root debug
    auth        required      /lib/security/pam_env.so
    auth        sufficient    /lib/security/pam_unix.so likeauth nullok
    auth        required      /lib/security/pam_deny.so

    account     required      /lib/security/pam_unix.so

    password    required      /lib/security/pam_cracklib.so retry=3 type=
    password    sufficient    /lib/security/pam_unix.so nullok use_authtok md5 shadow
    password    required      /lib/security/pam_deny.so

    session     sufficient    /lib/security/pam_afs.so set_token
    session     required      /lib/security/pam_limits.so
    session     required      /lib/security/pam_unix.so

    # reafslog is to unlock dtlogin's screensaver
    other  auth sufficient /usr/athena/lib/pam_krb4.so reafslog

### <a name="3.35 Setting up AFS key in KDC a"></a> 3.35 Setting up AFS key in KDC and [[KeyFile]]

The easiest method is not much recommended, as the password is possibly very weak:

    dattan# bos addkey server -kvno <n+1>
    <pw>
    <pw-again>

    dattan# kstring2key -c <cellname>
    <pw>

    dattan# kadmin ckey afs
    <cut-n-paste the ascii output from above>

Beware, there is currently a byte order bug in the kadmin tool as of May 2001 in KTH-KRB. If you run kadmin on a little indian machine (PC, Alpha, ...) you must swap the bytes manually. Here is a small example:

    bg$ kstring2key -c sics.se
    password:
    ascii = \361\334\211\221\221\206\325\334
    hex   = f1dc89919186d5dc
    bg$ kadmin
    kadmin: ckey afs
    bg.admin@SICS.SE's Password:
    New DES key for afs: \221\211\334\361\334\325\206\221

Please note how the bytes are swapped in groups of four.

This bug is fixed in an upcoming release.

Better approach is to create random key in KDC using "ksrvutil get" (rather then "ksrvutil add" which asks you for the new password), export it into /etc/srvtab ("ksrvutil ext") and from there copy it into /usr/afs/etc/KeyFile using asetkey utility from /afs/transarc.com/public/afs-contrib. After some years, you may wish to regenerate the random afs key using "ksrvutil change" in KDC and exporting it again.

There are two nice pages abou this:

<http://www.cs.cmu.edu/afs/andrew.cmu.edu/usr/shadow/www/afs/afs-with-kerberos.html> <http://www.contrib.andrew.cmu.edu/~shadow/afs/afs-with-kerberos.html>

As someone noted, you might need some ext\_srvtab utility to extract the key from /etc/srvtab, but I don't remeber that I needed it. Then he also suggets to use asetkey utility.

Create afs.natur\\.cuni\\.cz@NATUR.CUNI.CZ principal entry in kerberos:

      principal name: afs
      principal instance: natur.cuni.cz
      principal realm: NATUR.CUNI.CZ

Import this key from Kerberos (/etc/srvtab) into AFS /usr/afs/etc/KeyFile using asetkey utility, which is I think from /afs/transarc.com/public/afs-contrib

./asetkey add 0 /etc/srvtab <afs.natur.cuni.cz@NATUR.CUNI.CZ>

This [[KeyFile]] with the AFS-key you can just always re-copy to new AFS **server** machines. Be sure that the key version number KVNO is always same in this [[KeyFile]] and in Kerberos database. On all these machines you of course need afs.hostname.REALM key loaded into their /etc/srvtab (create them with 'ksrvutil get'). It seems klog(1) needs afs@REALM, so copy the principal afs.cell.name@REALM to it.

You can test if you have same keys in kerberos and AFS like this:

    kauth username
    tokens

If you have some tokens now, then it works and you can now shutdown kaserver. Users, which you have created in AFS under kaserver are in /usr/afs/db/kaserver.\*, but you can just forget them. Create these users again in Kerberos IV.

AFS users are then looked up in:

kerberos database (/var/kerberos/principal.db)

/usr/afs/etc/UserList (permission to manipulate 'bos')

system:administrators (permissions for vos, pts, fs, i.e. 'pts adduser' etc)

    ---------------------------------------------------------------------------------
    Another approach, untested:

    http://www.natur.cuni.cz/~majordomo/krb4/krb4.9703/msg00002.html

    Date: Tue, 9 May 2000 10:57:23 +0200 (MET DST)
    From: Johan Hedin <johanh  at  fusion.kth.se>
    To: Erland Fristedt <Erland.Fristedt@wcs.eraj.ericsson.se>
    Cc: krb4@sics.se
    Subject: Re: AFS+KTH-KRB problem

    This is a printout how we got it to work. Hope this helps.

    /Johan Hedin

    callisto#/usr/athena/sbin/ksrvutil -p johanh.admin -f srvtab get
    Name [rcmd]: afs
    Instance [callisto]: fusion.kth.se
    Realm [FUSION.KTH.SE]:
    Is this correct? (y,n) [y]
    Add more keys? (y,n) [n]
    Password for johanh.admin@FUSION.KTH.SE:
    Warning: Are you sure `fusion.kth.se' should not be `fusion'?
    Added afs.fusion.kth.se@FUSION.KTH.SE
    Old keyfile in srvtab.old.
    callisto#/usr/athena/sbin/ksrvutil -p johanh.admin -f srvtab list
    Version    Principal
       4     afs.fusion.kth.se@FUSION.KTH.SE
    callisto#/tmp/asetkey add 4 srvtab afs.fusion.kth.se@FUSION.KTH.SE
    callisto#cd ..
    callisto#bin/bos status callisto
    Instance buserver, currently running normally.
    Instance ptserver, currently running normally.
    Instance vlserver, currently running normally.
    Instance fs, currently running normally.
        Auxiliary status is: file server running.
    callisto#bin/fs la /afs
    Access list for /afs is
    Normal rights:

      system:administrators rlidwka

    --------------------------------------------------------------------------------
    > Tokens held by the Cache Manager:
    >
    > User's (AFS ID 6020) tokens for afs@sunrise.ericsson.se [Expires May  5
    > 04:05]
    >    --End of list--

    You got your tickets alright, but then

    > > touch /afs/.sunrise.ericsson.se/tabort
    > afs: Tokens for user of AFS id 6020 for cell sunrise.ericsson.se are
    > discarded (rxkad error=19270407)

    grep 19270407 /usr/afsws/include/rx/*
    /usr/afsws/include/rx/rxkad.h:#define RXKADBADTICKET (19270407L)

    so I would guess that the key version number and/or the key on your
    AFS servers does not match the key of the afs principal.

    Use klist -v to figure out about key version numbers and then use

    $ kstring2key -c sunrise.ericsson.se
    password:
    ascii = \242\345\345\260\323p\263\233
    hex   = a2e5e5b0d370b39b

    or some other means to ensure that you use the same keys.

### <a name="3.36 Obtaining and getting asetk"></a> 3.36 Obtaining and getting asetkey compiled

Better approach is to create random key in KDC, export it into /etc/srvtab and from there move in /usr/afs/etc/KeyFile using asetkey utility from /afs/transarc.com/public/afs-contrib

    |  I can not get asetkey to compile.
    |
    |  asetkey.c: In function `main':
    |  asetkey.c:25: `AFSCONF_SERVERNAME' undeclared (first use in this function)
    |  asetkey.c:25: (Each undeclared identifier is reported only once
    |  asetkey.c:25: for each function it appears in.)
    +--->8

AFS 3.5 got rid of a few commonly-used AFS defines; I suspect they are now API calls of some kind, akin to the POSIX move from hardcoded constants to use of \{,f\}pathconf() and sysconf(). I have no idea what those calls are, though.

In the meantime, the following two defines are useful in building stuff on AFS 3.5:

    #define AFSCONF_CLIENTNAME      "/usr/vice/etc"
    #define AFSCONF_SERVERNAME      "/usr/afs/etc"

### <a name="3.37 afs_krb_get_lrealm() using"></a><a name="3.37 afs_krb_get_lrealm() using "></a> 3.37 afs\_krb\_get\_lrealm() using /usr/afs/etc/krb.conf

In this file you can set also another REALM to be used by you afs server processes, if the REALM should differ from the system-wide REALM (

    /etc/krb.conf

).

Don't forget it's related to these entries in Kerberos KDC:

    afs.cell.name@REALM
    krbtgt CELL.NAME@REALM

If you use heimdal (or MIT krb5), do:

echo "REALM" &gt; /usr/afs/etc/krb.conf

### <a name="3.38 Moving from kaserver to Hei"></a> 3.38 Moving from kaserver to Heimdal KDC

    > While converting all our administration tools, we have discovered that
    > the time a principal changed his/her/its password is _not_ carried over
    > from the kaserver DB.

First of all, some Heimdal's configure flags:

    --enable-kaserver

requires krb4 libs, so for that you'll need a working krb4 are you still using a kaserver/kaserver emulation ?

    --enable-kaserver-db

is just for dumping a kaserver krb4 database. If you are no longer running a kaserver, you don't need it.

Migration itself:

<https://lists.openafs.org/pipermail/openafs-info/2002-May/004326.html>

This works while migrating from kaserver:

    /usr/heimdal/libexec/hprop --source=kaserver --cell=xxx
    --kaspecials --stdout | /usr/heimdal/libexec/hpropd --no-inetd --stdin

This somewhat doesn't:

    /usr/heimdal/libexec/hprop --source=kaserver --cell=xxx
    --encrypt --master-key=<path to key> --kaspecials --stdout |
    /usr/heimdal/libexec/hpropd --stdin

### <a name="3.39 Moving from KTH-KRB4 to Hei"></a> 3.39 Moving from KTH-KRB4 to Heimdal KDC

    /usr/heimdal/libexec/hprop -n -k /etc/krb5.keytab --source=krb4-dump -d /var/kerberos/dump.txt --master-key=/.k -D | /usr/heimdal/libexec/hpropd -n

    or

    1. dump of the krb4 database with kdb_util
    2. dump of the "default" heimdal database with kadmin -l
    3. /usr/heimdal/libexec/hprop -n -k /etc/krb5.keytab --source=krb4-dump -d /usr/heimdal/libexec/hprop -n -k /etc/krb5.keytab --source=krb4-dump -d
       where /etc/krb5.keytab contains hprop/`hostname` keys
    4. merge of the converted database with file from (2) via kadmin

    The special thing for me is the use of "-D" in the (3) which seems to
    cause conversion des-cbc-sha1 keys of old krb4 database entries to
    des-cbc-md5.

### <a name="3.40 What are those bos(1) -type"></a> 3.40 What are those bos(1) -type values simple and cron?

Typically, admins do this once they configure new afs server:

    bos create foo ptserver simple /usr/afs/bin/ptserver -cell bar.baz
    bos create foo vlserver simple /usr/afs/bin/vlserver -cell bar.baz
    bos create foo fs fs /usr/afs/bin/fileserver /usr/afs/bin/volserver /usr/afs/bin/salvager -cell bar.baz

Type "simple" has one process. type "cron" gets forked at the appropriate time.

### <a name="3.41 KDC listens on port 88 inst"></a> 3.41 KDC listens on port 88 instead of 750

    "Kramer, Matthew" <mattkr@bu.edu> writes:

    > Hello,
    >       Our Kerberos 4 environment listens on only port 750 and doesn't
    > listen on port 88.  For Win2K/XP machines this causes a problem in
    > C:\openafs-1.2.8\src\kauth\user_nt.c line 144 when the Kerberos port is
    > queried from the %systemroot%\system32\drivers\etc\services file.  Win2K
    > and XP both return 88 which is correct for Kerberos 5 but in our case
    > not for Kerberos 4.  Why doesn't it instead query for kerberos-iv which
    > would return the correct value of 750?
    >
    > Y:\src\kauth>diff user_nt.c.orig user_nt.c
    > 144c144
    > <       sp = getservbyname("kerberos", "udp");
    > ---
    > >       sp = getservbyname("kerberos-iv", "udp");
    >
    A quick work around could be to use Loves requestforwarder found at

    ftp://ftp.stacken.kth.se/pub/projects/krb-forward/krb-forward-0.1.tar.gz

    /JockeF

### <a name="3.42 afsd gives me &quot;Error -1 in"></a><a name="3.42 afsd gives me &quot;Error -1 in "></a> 3.42 afsd gives me "Error -1 in basic initialization." on startup

When starting afsd, I get the following:

    # /usr/vice/etc/afsd -nosettime -debug
    afsd: My home cell is 'foo.bar.baz'
    ParseCacheInfoFile: Opening cache info file '/usr/vice/etc/cacheinfo'...
    ParseCacheInfoFile: Cache info file successfully parsed:
           cacheMountDir: '/afs'
           cacheBaseDir: '/usr/vice/cache'
           cacheBlocks: 50000
    afsd: 5000 inode_for_V entries at 0x8075078, 20000 bytes
    SScall(137, 28, 17)=-1 afsd: Forking rx listener daemon.
    afsd: Forking rx callback listener.
    afsd: Forking rxevent daemon.
    SScall(137, 28, 48)=-1 SScall(137, 28, 0)=-1 SScall(137, 28, 36)=-1 afsd: Error -1 in basic initialization.

Solution: make sure you have kernel module loaded.

### <a name="3.43 Although I get krb tickets,"></a> 3.43 Although I get krb tickets, afslog doesn't give me tokens, I see UDP packets to port 4444

Using tcpdump I see the following traffic. What runs on the port 4444?

    15:16:54.815194 IP foo.bar.baz.32772 > foo.bar.baz.4444: UDP, length: 269
    15:16:54.815199 IP foo.bar.baz > foo.bar.baz: icmp 305: foo.bar.baz udp port 4444 unreachable

This is the 5-&gt;4 ticket conversion service on UDP port 4444. Assuming your afs servers know how to deal with krb5 tickets (since openafs-1.2.10? and 1.3.6x?) include the following in /etc/krb5.conf:

    [appdefaults]
            libkafs = {
                    afs-use-524 = local
            }

### <a name="3.44 I get error message trhough"></a> 3.44 I get error message trhough syslogd: "afs: Tokens for user of AFS id 0 for cell foo.bar.baz are discarded (rxkad error=19270407)"

Answer:

    grep 19270407 /usr/afsws/include/rx/*
    /usr/afsws/include/rx/rxkad.h:#define RXKADBADTICKET (19270407L)

### <a name="3.45 I get tickets and tokens, b"></a> 3.45 I get tickets and tokens, but still get Permission denied.

Answer: /usr/afs/etc/UserList accepts only krb4 syntax. Use `joe.admin` instead of `joe/admin`. See `https://lists.openafs.org/pipermail/openafs-devel/2002-December/008673.html` and the rest of the thread. [[DraGona]] [[DraGonb]] [[DraGonc]] [[DraGond]] [[DraGone]] [[DraGonf]] [[DraGong]] [[DraGonh]] [[DraGoni]] [[DraGonj]] [[DraGonk]] [[DraGonl]] [[DraGonm]] [[DraGonn]] [[DraGono]] [[DraGonp]] [[DraGonq]] [[DraGonr]] [[DraGons]] [[DraGont]] [[DraGonu]] [[DraGonv]] [[DraGonw]] [[DraGonx]] [[DraGony]] [[DraGonz]] [[DraGonaa]]

[网站推广](http://www.51dragon.com) [arm](http://1.51dragon.com) [光端机](http://guangduanji.51dragon.com) [数字光端机](http://guangduanji.51dragon.com/shuzi.htm) [视频光端机](http://guangduanji.51dragon.com/shipin.htm) [数字视频光端机](http://guangduanji.51dragon.com/szsp.htm) [监控视频光端机](http://guangduanji.51dragon.com/jiankong.htm) [广播级数字视频光端机](http://guangduanji.51dragon.com/guangbo.htm) [网络视频全套解决方案](http://guangduanji.51dragon.com/wangluo.htm) [arm](http://1.51dragon.com) [arm培训](http://2.51dragon.com) [安防](http://3.51dragon.com) [安防器材](http://3.51dragon.com/a.htm) [笔记本](http://4.51dragon.com) [笔记本电脑](http://5.51dragon.com) [变速机](http://6.51dragon.com) [标签](http://7.51dragon.com) [标签打印机](http://8.51dragon.com) [不孕不育](http://9.51dragon.com) [不孕不育治疗](http://9.51dragon.com/a.htm) [餐饮](http://10.51dragon.com) [餐饮管理](http://10.51dragon.com/a.htm) [叉车](http://11.51dragon.com) [磁性材料](http://12.51dragon.com) [刺绣](http://13.51dragon.com) [仓储](http://14.51dragon.com) [仓储设备](http://15.51dragon.com) [充电器](http://16.51dragon.com) [手机充电器](http://16.51dragon.com) [出国](http://17.51dragon.com) [除湿机](http://18.51dragon.com) [工业除湿机](http://18.51dragon.com/a.htm) [创业](http://19.51dragon.com) [创业项目](http://19.51dragon.com/a.htm) [床上用品](http://20.51dragon.com) [家居用品](http://20.51dragon.com/e.htm) [婴儿用品](http://20.51dragon.com/a.htm) [情趣用品](http://20.51dragon.com/b.htm) [宠物用品](http://20.51dragon.com/c.htm) [儿童用品](http://20.51dragon.com/d.htm) [单片机](http://21.51dragon.com) [打印机](http://22.51dragon.com) [灯具](http://23.51dragon.com) [电源](http://24.51dragon.com) [开关电源](http://25.51dragon.com) [ups电源](http://26.51dragon.com) [变频电源](http://27.51dragon.com) [稳压电源](http://28.51dragon.com) [电机](http://29.51dragon.com) [步进电机](http://30.51dragon.com) [微型电机](http://31.51dragon.com) [电梯](http://32.51dragon.com) [电子政务](http://33.51dragon.com) [雕塑](http://34.51dragon.com) [雕刻机](http://35.51dragon.com) [激光雕刻机](http://36.51dragon.com) [电脑雕刻机](http://36.51dragon.com/a.htm) [数控雕刻机](http://36.51dragon.com/b.htm) [橡胶版雕刻机](http://36.51dragon.com/c.htm) [木工雕刻机](http://36.51dragon.com/d.htm) [短信群发](http://37.51dragon.com) [对讲机](http://38.51dragon.com) [无线对讲机](http://38.51dragon.com/a.htm) [erp](http://39.51dragon.com) [erp软件](http://40.51dragon.com) [耳机](http://41.51dragon.com) [无线耳机](http://41.51dragon.com/a.htm) [耳聋](http://42.51dragon.com) [阀](http://43.51dragon.com) [阀门](http://44.51dragon.com) [球阀](http://44.51dragon.com/a.htm) [法律咨询](http://45.51dragon.com) [翻译](http://46.51dragon.com) [翻译公司 ](http://47.51dragon.com) [北京翻译](http://48.51dragon.com) [纺织机械](http://49.51dragon.com) [服装](http://50.51dragon.com) [服务器](http://51.51dragon.com) [gprs](http://52.51dragon.com) [gps](http://53.51dragon.com) [车载gps](http://54.51dragon.com) [防盗GPS](http://55.51dragon.com) [干燥](http://56.51dragon.com) [干燥机](http://57.51dragon.com) [干燥设备](http://58.51dragon.com) [干洗](http://59.51dragon.com) [干洗设备](http://60.51dragon.com) [钢结构](http://61.51dragon.com) [钢铁](http://62.51dragon.com) [钢材](http://63.51dragon.com) [工控](http://64.51dragon.com) [工控机](http://65.51dragon.com) [公寓](http://66.51dragon.com) [管理咨询](http://67.51dragon.com) [管理培训](http://68.51dragon.com) [项目管理](http://69.51dragon.com) [管理顾问](http://70.51dragon.com) [光触媒](http://71.51dragon.com) [尖锐湿疣](http://72.51dragon.com) [广告](http://73.51dragon.com) [广告策划](http://74.51dragon.com) [广告设计](http://75.51dragon.com) [耗材](http://76.51dragon.com) [办公耗材](http://77.51dragon.com) [化工原料](http://78.51dragon.com) [化工设备](http://79.51dragon.com) [滑雪](http://80.51dragon.com) [滑雪场](http://81.51dragon.com) [化妆品](http://82.51dragon.com) [换热器](http://83.51dragon.com) [板式换热器](http://83.51dragon.com/a.htm) [波纹管换热器](http://83.51dragon.com/b.htm) [螺旋板换热器](http://83.51dragon.com/c.htm) [列管换热器](http://83.51dragon.com/d.htm) [热管换热器](http://83.51dragon.com/e.htm) [舒瑞普板式换热器](http://83.51dragon.com/f.htm) [石墨换热器](http://83.51dragon.com/g.htm) [盘管换热器](http://83.51dragon.com/h.htm) [半导体换热器](http://83.51dragon.com/i.htm) [婚庆](http://84.51dragon.com) [婚庆公司](http://85.51dragon.com) [货运](http://86.51dragon.com) [货运公司](http://87.51dragon.com) [货架](http://88.51dragon.com) [仓储货架](http://89.51dragon.com) [物流仓储货架](http://89.51dragon.com/a.htm) [立体仓储](http://89.51dragon.com/b.htm) [移动货架](http://89.51dragon.com/c.htm) [角钢货架](http://89.51dragon.com/d.htm) [集团电话](http://90.51dragon.com) [集团电话交换机](http://90.51dragon.com/a.htm) [机柜](http://91.51dragon.com) [机床](http://92.51dragon.com) [机箱](http://93.51dragon.com) [机票](http://94.51dragon.com) [打折机票](http://95.51dragon.com) [特价机票](http://96.51dragon.com) [国际机票](http://97.51dragon.com) [订机票](http://98.51dragon.com) [机票价格](http://99.51dragon.com) [飞机票](http://100.51dragon.com) [继电器](http://101.51dragon.com) [固态继电器](http://102.51dragon.com) [时间继电器](http://102.51dragon.com/a.htm) [热继电器](http://102.51dragon.com/b.htm) [中间继电器](http://102.51dragon.com/c.htm) [小型继电器](http://102.51dragon.com/d.htm) [接地继电器](http://102.51dragon.com/e.htm) [汽车继电器](http://102.51dragon.com/f.htm) [交流继电器](http://102.51dragon.com/g.htm) [加盟](http://103.51dragon.com) [加盟店](http://104.51dragon.com) [连锁加盟](http://105.51dragon.com) [家具](http://106.51dragon.com) [办公家具](http://107.51dragon.com) [驾校](http://108.51dragon.com) [北京驾校](http://108.51dragon.com/a.htm) [家政](http://109.51dragon.com) [家政服务](http://110.51dragon.com) [减肥](http://111.51dragon.com) [监控](http://112.51dragon.com) [闭路监控](http://113.51dragon.com) [远程监控](http://114.51dragon.com) [监控设备](http://115.51dragon.com) [交友](http://116.51dragon.com) [交换机](http://117.51dragon.com) [程控交换机](http://118.51dragon.com) [电话交换机](http://119.51dragon.com) [网络交换机](http://120.51dragon.com) [建材](http://121.51dragon.com) [新型建材](http://122.51dragon.com) [酒店](http://123.51dragon.com) [酒店预定](http://124.51dragon.com) [酒店预订](http://125.51dragon.com) [KVM](http://126.51dragon.com) [切换器](http://127.51dragon.com) [矩阵切换器](http://128.51dragon.com) [考勤机](http://129.51dragon.com) [指纹](http://130.51dragon.com) [指纹考勤机](http://131.51dragon.com) [巡更](http://132.51dragon.com) [电子巡更](http://132.51dragon.com/a.htm) [空压机](http://133.51dragon.com) [螺杆式空压机](http://133.51dragon.com/a.htm) [阿特拉斯空压机](http://133.51dragon.com/b.htm) [活塞式空压机](http://133.51dragon.com/c.htm) [螺杆空压机](http://133.51dragon.com/d.htm) [空压机配件](http://133.51dragon.com/e.htm) [移动式空压机](http://133.51dragon.com/f.htm) [单螺杆空压机](http://133.51dragon.com/g.htm) [进口空压机](http://133.51dragon.com/h.htm) [礼品](http://134.51dragon.com) [工艺品](http://135.51dragon.com) [纪念品](http://136.51dragon.com) [礼品公司](http://137.51dragon.com) [猎头](http://138.51dragon.com) [猎头公司](http://139.51dragon.com) [留学](http://140.51dragon.com) [法国留学](http://141.51dragon.com) [英国留学](http://142.51dragon.com) [德国留学](http://143.51dragon.com) [美国留学](http://144.51dragon.com) [出国留学](http://145.51dragon.com) [留学签证](http://146.51dragon.com) [隆胸](http://147.51dragon.com) [路由器](http://148.51dragon.com) [旅行社](http://149.51dragon.com) [律师](http://150.51dragon.com) [律师事务所](http://151.51dragon.com) [mba](http://152.51dragon.com) [emba](http://153.51dragon.com) [美容](http://154.51dragon.com) [整形美容](http://155.51dragon.com) [门禁](http://156.51dragon.com) [一卡通](http://157.51dragon.com) [停车场](http://158.51dragon.com) [门禁系统](http://159.51dragon.com) [指纹门禁](http://159.51dragon.com/a.htm) [门禁系统](http://159.51dragon.com/b.htm) [门禁考勤](http://159.51dragon.com/c.htm) [门禁管理系统](http://159.51dragon.com/e.htm) [门禁卡](http://159.51dragon.com/f.htm) [酒店门禁](http://159.51dragon.com/g.htm) [电子门禁](http://159.51dragon.com/h.htm) [门禁设备](http://159.51dragon.com/i.htm) [门禁软件](http://159.51dragon.com/j.htm) [模具](http://160.51dragon.com) [五金模具](http://160.51dragon.com/a.htm) [塑料模具](http://161.51dragon.com) [排队机](http://162.51dragon.com) [票务](http://163.51dragon.com) [培训](http://164.51dragon.com) [IT培训](http://165.51dragon.com) [计算机培训](http://166.51dragon.com) [软件培训](http://167.51dragon.com) [汽车](http://168.51dragon.com) [汽车美容](http://169.51dragon.com) [汽车配件](http://170.51dragon.com) [二手车](http://171.51dragon.com) [汽车养护](http://172.51dragon.com) [汽车租赁](http://173.51dragon.com) [起名](http://174.51dragon.com) [签证](http://175.51dragon.com) [求职](http://176.51dragon.com) [招聘](http://177.51dragon.com) [热水器](http://178.51dragon.com) [太阳能热水器](http://178.51dragon.com/a.htm) [人力资源](http://179.51dragon.com) [人力资源管理](http://179.51dragon.com/a.htm) [润滑油](http://180.51dragon.com) [商标](http://181.51dragon.com) [商标注册](http://182.51dragon.com) [首饰](http://183.51dragon.com) [设计](http://184.51dragon.com) [包装设计](http://185.51dragon.com) [平面设计](http://186.51dragon.com) [摄像机](http://187.51dragon.com) [数码摄像机](http://188.51dragon.com) [石材](http://189.51dragon.com) [石材养护](http://189.51dragon.com/a.htm) [视频会议](http://190.51dragon.com) [视频会议设备](http://191.51dragon.com) [视频会议系统](http://192.51dragon.com) [数据修复](http://193.51dragon.com) [水处理](http://194.51dragon.com) [水处理设备](http://195.51dragon.com) [水泵](http://196.51dragon.com) [真空泵](http://197.51dragon.com) [电力真空泵](http://197.51dragon.com/a.htm) [速记](http://198.51dragon.com) [速记培训](http://199.51dragon.com) [塑料](http://200.51dragon.com) [塑料机械](http://201.51dragon.com) [手表](http://202.51dragon.com) [瑞士手表](http://202.51dragon.com/a.htm) [卡西欧手表](http://202.51dragon.com/b.htm) [浪琴手表](http://202.51dragon.com/c.htm) [手表报价](http://202.51dragon.com/d.htm) [手表品牌](http://202.51dragon.com/e.htm) [男士手表](http://202.51dragon.com/f.htm) [女士手表](http://202.51dragon.com/g.htm) [情侣手表](http://202.51dragon.com/h.htm) [劳力士手表](http://202.51dragon.com/i.htm) [时尚手表](http://202.51dragon.com/j.htm) [陶瓷](http://203.51dragon.com) [陶瓷雕塑](http://203.51dragon.com/a.htm) [压电陶瓷](http://203.51dragon.com/b.htm) [陶瓷制品](http://203.51dragon.com/c.htm) [陶瓷膜](http://203.51dragon.com/e.htm) [条码](http://204.51dragon.com) [条码打印机](http://205.51dragon.com) [投影机](http://206.51dragon.com) [投影机维修](http://206.51dragon.com/a.htm) [大屏幕](http://207.51dragon.com) [投资](http://208.51dragon.com) [投资项目](http://209.51dragon.com) [涂料](http://210.51dragon.com) [团购](http://211.51dragon.com) [汽车团购](http://211.51dragon.com/a.htm) [无忧团购](http://211.51dragon.com/b.htm) [团购网](http://211.51dragon.com/c.htm) [北京团购](http://211.51dragon.com/d.htm) [建材团购](http://211.51dragon.com/e.htm) [家具团购](http://211.51dragon.com/f.htm) [年货团购](http://211.51dragon.com/g.htm) [团购服务](http://211.51dragon.com/h.htm) [服装团购](http://211.51dragon.com/i.htm) [托盘](http://212.51dragon.com) [VOD](http://213.51dragon.com) [VOIP](http://214.51dragon.com) [挖掘机](http://215.51dragon.com) [二手挖掘机](http://215.51dragon.com/a.htm) [挖掘机配件](http://215.51dragon.com/b.htm) [网络挖掘机](http://215.51dragon.com/c.htm) [挖掘机械](http://215.51dragon.com/d.htm) [挖掘机修理](http://215.51dragon.com/e.htm) [物流](http://216.51dragon.com) [玩具](http://217.51dragon.com) [儿童玩具](http://217.51dragon.com/a.htm) [鲜花](http://218.51dragon.com) [北京鲜花](http://218.51dragon.com/a.htm) [鲜花速递](http://219.51dragon.com) [北京鲜花速递](http://219.51dragon.com/a.htm) [显示器](http://220.51dragon.com) [显示器维修](http://221.51dragon.com/) [小提琴](http://222.51dragon.com) [小尾羊](http://223.51dragon.com) [相机](http://224.51dragon.com) [帖纸相机](http://224.51dragon.com/a.htm) [数码相机](http://224.51dragon.com/b.htm) [虚拟主机](http://225.51dragon.com) [北京虚拟主机](http://225.51dragon.com/a.htm) [空间租用](http://226.51dragon.com) [虚拟主机提供商](http://226.51dragon.com/a.htm) [虚拟主机租用](http://226.51dragon.com/b.htm) [域名虚拟主机](http://226.51dragon.com/c.htm) [虚拟主机空间](http://226.51dragon.com/d.htm) [雅思](http://227.51dragon.com) [雅思考试](http://227.51dragon.com/a.htm) [雅思报名](http://227.51dragon.com/b.htm) [液压](http://228.51dragon.com) [液压机](http://228.51dragon.com/a.htm) [液压设备](http://228.51dragon.com/b.htm) [液压件](http://228.51dragon.com/c.htm) [移民](http://229.51dragon.com) [加拿大移民](http://229.51dragon.com/a.htm) [新加坡移民](http://229.51dragon.com/b.htm) [音响](http://230.51dragon.com) [音箱](http://231.51dragon.com) [印刷机械](http://232.51dragon.com) [印刷设备](http://233.51dragon.com) [印刷机](http://234.51dragon.com) [英语培训](http://235.51dragon.com) [外语培训](http://236.51dragon.com) [法语培训](http://237.51dragon.com) [饮水机](http://238.51dragon.com) [元器件](http://239.51dragon.com) [电子元器件](http://240.51dragon.com) [油漆](http://241.51dragon.com) [展览](http://242.51dragon.com) [展览展示](http://243.51dragon.com) [整形](http://244.51dragon.com) [整形手术](http://245.51dragon.com) [制药机械](http://246.51dragon.com) [招商](http://247.51dragon.com) [药品招商](http://248.51dragon.com) [医药招商](http://249.51dragon.com) [知识产权](http://250.51dragon.com) [智能家居](http://251.51dragon.com) [轴承](http://252.51dragon.com) [微型轴承](http://252.51dragon.com/a.htm) [珠宝](http://253.51dragon.com) [注册公司](http://254.51dragon.com) [公司注册](http://255.51dragon.com) [装饰](http://256.51dragon.com) [装饰设计](http://257.51dragon.com) [装修](http://258.51dragon.com) [家庭装修](http://258.51dragon.com/a.htm) [室内装修](http://258.51dragon.com/b.htm) [家居装修](http://258.51dragon.com/c.htm) [房屋装修](http://258.51dragon.com/d.htm) [装饰装修](http://258.51dragon.com/e.htm) [装修公司](http://258.51dragon.com/f.htm) [装修设计](http://258.51dragon.com/g.htm) [厨房装修](http://258.51dragon.com/h.htm) [居室装修](http://258.51dragon.com/i.htm) [住宅装修](http://258.51dragon.com/j.htm) [住房装修](http://258.51dragon.com/k.htm) [租房](http://259.51dragon.com) [北京租房](http://260.51dragon.com) [二手房](http://260.51dragon.com/a.htm) [旅游](http://261.51dragon.com) [云南旅游](http://262.51dragon.com) [海南旅游](http://263.51dragon.com) [三亚](http://264.51dragon.com) [三亚旅游](http://265.51dragon.com) [四川旅游](http://266.51dragon.com) [张家界](http://267.51dragon.com) [张家界旅游](http://268.51dragon.com) [旅游景点](http://269.51dragon.com) [香港旅游](http://270.51dragon.com) [桂林旅游](http://271.51dragon.com) [桂林](http://272.51dragon.com) [猎头](http://273.51dragon.com) [猎头公司](http://274.51dragon.com) [OA](http://275.51dragon.com) [办公自动化](http://276.51dragon.com) [九寨沟](http://277.51dragon.com) [九寨沟旅游](http://278.51dragon.com) [欧洲旅游](http://279.51dragon.com) [三峡](http://280.51dragon.com) [三峡旅游](http://281.51dragon.com) [调查](http://282.51dragon.com) [侦探](http://283.51dragon.com) [私人侦探](http://284.51dragon.com) [私家侦探](http://285.51dragon.com) [调查公司](http://286.51dragon.com) [市场调查](http://287.51dragon.com) [泵](http://288.51dragon.com) [水泵](http://289.51dragon.com) [真空泵](http://290.51dragon.com) [mcse](http://291.51dragon.com) [ccnp](http://292.51dragon.com) [ccna](http://293.51dragon.com) [专利](http://294.51dragon.com) [smt](http://295.51dragon.com) [房地产](http://296.51dragon.com) [橡胶](http://297.51dragon.com) [糖尿病](http://298.51dragon.com) [白癜风](http://299.51dragon.com) [氢氧化铝](http://300.51dragon.com) [氢氧化镁](http://300.51dragon.com/a.htm) [水镁石](http://300.51dragon.com/b.htm) [萤石](http://300.51dragon.com/c.htm) [制氮机](http://301.51dragon.com) [发电机](http://302.51dragon.com) [发电机组](http://303.51dragon.com) [IBM笔记本](http://304.51dragon.com) [索尼笔记本](http://305.51dragon.com) [流量计](http://306.51dragon.com) [光触媒](http://307.51dragon.com) [DELL笔记本](http://308.51dragon.com) [流媒体](http://309.51dragon.com) [流媒体编码卡](http://310.51dragon.com) [多屏卡](http://311.51dragon.com) [视频墙](http://312.51dragon.com) [切换台](http://313.51dragon.com) [转换盒](http://314.51dragon.com) [视频采集卡](http://315.51dragon.com) [视频卡](http://316.51dragon.com) [特技切换台](http://317.51dragon.com) [数模转换器](http://318.51dragon.com) [大屏幕控制器](http://319.51dragon.com) [包装机械](http://320.51dragon.com) [包装机](http://321.51dragon.com) [液晶电视](http://322.51dragon.com) [等离子电视](http://323.51dragon.com) [液晶显示器](http://324.51dragon.com) [等离子显示器](http://325.51dragon.com) [IBM显示器](http://326.51dragon.com) [IBM工作站](http://327.51dragon.com) [SONY显示器](http://328.51dragon.com) [显示设备](http://329.51dragon.com) [乙肝](http://yigan.51dragon.com) [肝病](http://ganbing.51dragon.com) [肝炎](http://ganyan.51dragon.com) [物流认证](http://wuliurenzheng.51dragon.com) [物流培训](http://wuliupeixun.51dragon.com)
